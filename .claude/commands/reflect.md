# /reflect - Session Reflection Command

## Purpose

Generate a structured, reflective analysis of the current Claude Code session focusing on **HOW work was done**, not just **WHAT was accomplished**.

This command helps practitioners:
- Identify effective techniques and approaches
- Recognize patterns that worked well or need improvement
- Document lessons learned for future sessions
- Build a personal knowledge base of meta-work insights

## Invocation

```
/reflect [--focus AREA] [--name SLUG]
```

### Parameters

- `--focus AREA` (optional): Focus reflection on a specific area
  - `tools`: Tool usage patterns and effectiveness
  - `communication`: Clarification and iteration patterns
  - `planning`: Planning and execution strategy
  - `patterns`: Problem-solving techniques and approaches
  - Default: all areas

- `--name SLUG` (optional): Custom kebab-case slug for filename
  - Default: auto-generated from session gist

## Workflow

### Step 1: Analyze the Session

Review the **entire conversation history** and identify:

#### Techniques & Approaches
- What problem-solving methods were used?
- Did the AI explore multiple approaches or pick one quickly?
- Were there pivots or course corrections? Why?
- Which techniques were effective vs. inefficient?

#### Tool Usage
- Which tools were used and why?
- Were tools used optimally? (e.g., Grep vs. Bash vs. Task agents)
- Were any tools underutilized?
- Any surprising or creative tool combinations?

#### Communication Patterns
- How were requirements clarified?
- Did the user provide clear direction or did it evolve?
- How many iterations occurred? Were they productive?
- Any communication breakdowns or assumptions?

#### Planning & Execution
- Was the work planned upfront or emergent?
- Did the plan survive contact with reality?
- How was complexity managed?
- Were there unexpected blockers?

#### Tool Chains & Workflows
- How did tools chain together?
- Could the workflow be improved?
- Were there manual steps that could be automated?

### Step 2: Generate Insights

**What Went Well**
- Identify 3-5 specific techniques, tool usage patterns, or communication approaches that were effective
- Be concrete: "Used Explore agent to map codebase before making edits" not "worked efficiently"
- Explain why it worked

**What Went Wrong**
- Identify approaches that caused delays, confusion, or inefficiency
- Include tool misuse, overcomplicated workflows, or communication gaps
- Be honest about false starts and backtracking

**Lessons Learned**
- Extract 2-4 actionable insights from the session
- Connect observations to broader principles
- Frame as "I/we learned that..." statements
- Make them specific enough to guide future behavior

**Action Items**
- Concrete, testable improvements for future sessions
- Should be directly implementable
- Prioritize by impact

**Tips & Tricks**
- Document 2-3 specific Claude Code patterns discovered or reinforced
- Include keyboard shortcuts, MCP server features, or tool combos
- Make them reusable for future sessions

### Step 3: Create the Reflection File

**Directory Structure**

Files are created in:
```
.claude/
└── reflections/
    └── YYYY-MM-DD-slug.md
```

**Filename Pattern**

- `YYYY-MM-DD`: Today's date (e.g., `2025-01-12`)
- `slug`: 2-4 word kebab-case derived from session gist
  - Examples: `api-debugging`, `test-refactoring`, `feature-planning`, `code-analysis`, `template-review`

**File Content**

```markdown
# Session Reflection: [Brief Title]

**Date**: YYYY-MM-DD
**Session Goal**: [One-line summary of what this session aimed to accomplish]

---

## What Went Well

- [Specific effective technique/tool/communication pattern]
  - Why it worked: [Explanation]
- [Another example]
- [Another example]

## What Went Wrong

- [Approach that didn't work or caused delays]
  - Why it was inefficient: [Explanation]
- [Another example]
- [Another example]

## Lessons Learned

1. **[Lesson Title]**: [Explanation of the insight and why it matters for future work]
2. **[Lesson Title]**: [Explanation]
3. **[Lesson Title]**: [Explanation]

## Action Items

- [ ] [Specific improvement to apply in future sessions] (Priority: High/Medium/Low)
- [ ] [Another actionable change]

## Tips & Tricks for Claude Code

Based on this session, useful patterns for future reference:

- **Tip**: [Specific Claude Code feature or pattern discovered]
  - How to use: [Brief explanation]
- **Tip**: [Another useful pattern]

## Generalization Opportunities

[Only include if applicable. Be honest about whether patterns are truly reusable]

- **Slash Command**: [If a multi-step workflow could be automated]
- **Agent**: [If a specialized task type emerged]
- **Skill**: [If a reusable guide would help]

---

*Generated by `/reflect` command*
```

---

## Quality Checklist

Before finalizing the reflection, verify:

- [ ] **Specific, not vague** - "Used Explore agent to find 12 UDFs in 2 seconds instead of grepping manually" not "worked efficiently"
- [ ] **Honest** - Includes real shortcomings and false starts, not just wins
- [ ] **Actionable** - Each lesson and action item can guide future behavior
- [ ] **Concise** - 500-800 words total; quality over quantity
- [ ] **Process-focused** - About HOW work happened, not WHAT was delivered
- [ ] **Grounded** - References specific moments from conversation

---

## Examples

### Example 1: Code Analysis Session

```
# Session Reflection: PowerApp Code Review

**Date**: 2025-01-12
**Session Goal**: Analyze PowerApps repository structure and patterns for educational breakdown

## What Went Well

- **Parallel Agent Exploration**: Launched 3 Explore agents simultaneously to cover different analysis areas (architecture, error handling, delegation). This reduced analysis time by ~60% vs. sequential exploration. The agents had clear, specific prompts focusing each one on a different aspect.

- **Direct File Reading**: Rather than relying solely on Grep/Glob, read key source files (App-Formulas-Template.fx, App-OnStart-Minimal.fx) to understand real implementation vs. theoretical patterns. This revealed inconsistencies between documented patterns and actual code.

- **Systematic Inconsistency Tracking**: Created a summary table of code issues with severity, impact, and fix effort. This made it easy to prioritize which issues to fix and communicated scope clearly.

## What Went Wrong

- **Over-Planning Before Action**: Spent significant time planning the educational breakdown before validating against actual code. Would have been faster to explore code first, then plan based on findings.

- **Initial CLAUDE.md Review**: Spent time analyzing the existing CLAUDE.md against my proposed improved version. Could have just written the better version and asked for comparison.

## Lessons Learned

1. **Parallel Agent Exploration is Powerful for Analysis**: When analyzing a complex codebase with multiple distinct areas, launching specialized agents in parallel is more efficient than sequential exploration. The key is ensuring each agent has a specific, non-overlapping focus.

2. **Reading Source Files Beats Documentation**: Template documentation can lag behind actual implementation. Always read a few key source files to validate patterns before creating guides.

3. **Inconsistency Tracking Adds Clarity**: Creating a structured issue table (problem, file, line, severity, fix) made fixing priorities obvious and communicated scope to the user.

## Action Items

- [ ] Use parallel Explore agents as default for multi-area code analysis (Priority: High)
- [ ] Always read 2-3 source files early in analysis phase (Priority: High)
- [ ] Create summary tables for issues before proposing fixes (Priority: Medium)

## Tips & Tricks for Claude Code

- **Parallel Agent Exploration**: Launch up to 3 Explore agents with specific sub-prompts simultaneously to analyze different areas of a codebase. Much faster than sequential exploration.

- **Source File Validation**: For complex domains, read key source files early. The actual implementation reveals patterns and gotchas that documentation misses.

- **Issue Summary Table**: Use a structured table format (File | Line | Severity | Impact) when reporting multiple issues. Makes prioritization and communication clear.

---

*Generated by `/reflect` command*
```

---

## Integration with Workflow

### Using Reflections

1. **After Major Work Sessions**: Run `/reflect` to capture learnings while fresh
2. **Before Similar Tasks**: Read previous reflections on similar work types
3. **Quarterly Review**: Compile reflections to identify trends in effective approaches
4. **Knowledge Sharing**: Share particularly insightful reflections with team/mentees

### Archiving

Keep all reflections in `.claude/reflections/`:
- They create a personal knowledge base over time
- Patterns emerge when you have 10+ reflections
- Reference them in future sessions: "Similar to the `2025-01-12-code-analysis` reflection..."

---

## Notes for Claude (AI Assistant)

When invoked, this command asks you to:

1. **Carefully review the conversation history** - Identify specific techniques, tool usage, communication patterns
2. **Be concrete and specific** - Not "worked well" but "used Explore agent with 3 specialized sub-prompts in parallel, reducing analysis time by ~60%"
3. **Include honest shortcomings** - Not just successes; false starts and inefficiencies are the most valuable learning
4. **Extract actionable insights** - Each lesson should directly inform future behavior
5. **Focus on process/HOW, not outcome/WHAT** - The reflection is about work methodology, not the deliverable
6. **Create the file in .claude/reflections/** with date + kebab-case slug filename

Make the reflection concise (500-800 words), honest, and specific enough that reading it 3 months later will still be useful for avoiding past mistakes and repeating effective approaches.
